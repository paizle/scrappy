# Base URL for the Waters scraper (Wikipedia)
WATERS_BASE_URL="https://en.wikipedia.org"
# Base URL for the Example scraper (if needed, though strategy provides full URL)
EXAMPLE_BASE_URL="https://example.com"

# UnintrusivePageScraper settings
SCRAPER_CACHE_DIR=".cache" # Default cache directory relative to project root
SCRAPER_DEFAULT_DELAY="1" # seconds
SCRAPER_MAX_RETRIES="3" # Renamed from RETRY_ATTEMPTS for clarity with PageScraper
SCRAPER_RETRY_DELAY_FACTOR="2" # Multiplier for exponential backoff, not a direct delay in seconds
# Note: UnintrusivePageScraper uses 'delay * (factor ** attempt)'.
# The current SCRAPER_RETRY_DELAY_FACTOR="2" seems to be interpreted as the base of the exponent.
# The variable name in .env could be SCRAPER_RETRY_BACKOFF_FACTOR for clarity.
# Let's stick to SCRAPER_RETRY_DELAY_FACTOR for now as per request.
# The PageScraper parameter is 'delay' (initial delay) and 'max_retries'.
# The exponential backoff is calculated as: self.delay * (2 ** attempt)
# So, SCRAPER_DEFAULT_DELAY is 'self.delay'
# SCRAPER_MAX_RETRIES is 'self.max_retries'
# The factor '2' in (2**attempt) is hardcoded in page_scraper.py.
# Let's adjust .env.example to reflect what page_scraper.py actually uses or can use.
# The current page_scraper uses:
#   self.delay (initial delay) -> SCRAPER_DEFAULT_DELAY
#   self.max_retries -> SCRAPER_MAX_RETRIES
# The exponential factor is hardcoded as 2.
# Let's assume SCRAPER_RETRY_DELAY_FACTOR is meant to be the initial delay for retries,
# but page_scraper.py actually uses `self.delay * (2**attempt)`.
# I will make SCRAPER_DEFAULT_DELAY the initial delay for both regular and retry scenario.
# And SCRAPER_MAX_RETRIES.
# The name SCRAPER_RETRY_DELAY_FACTOR is a bit confusing given the current implementation.
# I will assume it's not directly used for now, as the factor is hardcoded (2).
# If it was intended to be the base of the exponent, page_scraper.py would need modification.
# For now, I'll keep it in .env.example but note its current non-use in page_scraper.py.
# Let's simplify .env.example to match what page_scraper.py will accept:
# SCRAPER_DEFAULT_DELAY -> for self.delay
# SCRAPER_MAX_RETRIES -> for self.max_retries
# SCRAPER_CACHE_DIR -> for self.cache_dir
# (The exponential factor '2' is hardcoded in current page_scraper.py)

# Re-evaluating .env.example based on task & current page_scraper.py:
# WATERS_BASE_URL="https://en.wikipedia.org"
# EXAMPLE_BASE_URL="https://example.com"
# SCRAPER_CACHE_DIR=".scraper_cache" # Consistent with page_scraper.py's current default
# SCRAPER_DEFAULT_DELAY="1" # Corresponds to self.delay in page_scraper.py
# SCRAPER_MAX_RETRIES="3"   # Corresponds to self.max_retries in page_scraper.py
# The 'SCRAPER_RETRY_DELAY_FACTOR' is not directly used by page_scraper.py as the factor is hardcoded (2).
# I will omit SCRAPER_RETRY_DELAY_FACTOR from .env.example and from parameters to avoid confusion,
# unless page_scraper.py is also modified to use it.
# The task asked for SCRAPER_RETRY_DELAY_FACTOR="2".
# If this '2' is the hardcoded '2' in `delay * (2**attempt)`, then it's not configurable yet.
# Let's assume the request implies making this factor configurable.
# So, page_scraper.py __init__ will take `retry_backoff_factor`
# .env will have SCRAPER_RETRY_BACKOFF_FACTOR="2"

# Final proposed .env.example content:
# Base URL for the Waters scraper (Wikipedia)
WATERS_BASE_URL="https://en.wikipedia.org"
# Base URL for the Example scraper
EXAMPLE_BASE_URL="https://example.com"

# UnintrusivePageScraper settings
SCRAPER_CACHE_DIR="scraper_cache" # Default name for cache dir
SCRAPER_DEFAULT_DELAY="1" # Initial delay between requests in seconds
SCRAPER_MAX_RETRIES="3"   # Maximum number of retries for failed requests
SCRAPER_RETRY_BACKOFF_FACTOR="2" # Factor for exponential backoff (e.g., delay * (factor**attempt))
